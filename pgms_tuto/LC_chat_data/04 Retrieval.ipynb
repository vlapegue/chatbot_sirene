{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow.\n",
    "# On charge le vectorDB de la leçon précédente\n",
    "# On va voir comment traiter les petits bugs qui diminuent la performance du retrieval\n",
    "\n",
    "# Parfois, ce ne sont pas les vecteurs les plus similaires à la question qui sont les plus pertinents\n",
    "# Concept de Maximum marginal relevance : on prend les k plus similaires, \n",
    "# puis parmi ces derniers on prend les k' plus diversifiés\n",
    "\n",
    "# Autre type de retrieval : LLM aided retrieval\n",
    "# La question posée fournit de éléments de contexte importants (SelfQuery), que le LLM peut repérer\n",
    "# La plupart des vectorStores ont un filtre sur les metadata, le LLM peut s'en servir\n",
    "\n",
    "# Dernière technique : Compression, pour ne retenir que la partie qui nous intéresse du document suggéré par le retrieval\n",
    "# Mais comme on utilise le LLM pour faire le choix, ça prend du temps à l'inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d24167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f315cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a bien le nombre de splits initial\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745a2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57169494",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4314d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_k = nombre de documents donc on ne fait pas de sélection\n",
    "# Ici c'est utile pour récupérer l'info qu'on voulait\n",
    "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "# Last class we introduced one problem: how to enforce diversity in the search results.\n",
    " \n",
    "# `Maximum marginal relevance` strives to achieve both relevance to the query *and diversity*\n",
    "# among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637df31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fff295",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the difference in results with `MMR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f15442",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed32cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Addressing Specificity: working with metadata\n",
    "\n",
    "#In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n",
    "\n",
    "#To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "#`metadata` provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe55a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b886402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique un filtre qui va regarder dans les metadata\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e965cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing Specificity: working with metadata using self-query retriever\n",
    "# But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "# To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "# 1. The `query` string to use for vector search\n",
    "# 2. A metadata filter to pass in as well\n",
    "# Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c94af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "# On importe un retriever spécial qui tient compte de la séparation de la query en \n",
    "# question et contexte\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "# AttributeInfo va rechercher l'info dans les metadata\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1bf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important de bien remplir \"description\" vu que ce sera lu par le LLM\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "llm = OpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will receive a warning about predict_and_parse being deprecated the first time you executing the next line. \n",
    "# This can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a mis verbose=True, donc on a une explication sur ce que le modèle fait\n",
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4202e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional tricks : compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach for improving the quality of retrieved docs is compression.\n",
    "# Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
    "# Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "# Contextual compression is meant to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5992668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c403b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afee165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En ajoutant search_type=mmr, on peut combiner l'intérêt de MMR et compression\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd00324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other types of retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's worth noting that vectordb as not the only kind of tool to retrieve documents.\n",
    "# Il y a en effet d'autres techniques NLP plus traditionnelles\n",
    "# The LangChain retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée des pipelines avec SVM et TfIdf\n",
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c65f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12be2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2759f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace3c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ee0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0cf6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9de5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f85e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
