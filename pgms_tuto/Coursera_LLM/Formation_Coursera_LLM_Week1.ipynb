{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82724c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objet du lab1 : résumer des dialogues\n",
    "# et se familiariser avec le prompt engineering (zero shot, one shot, few shot inference)\n",
    "# Le prompt engineering permet de tester à moindre coût la performance d'un modèle sur une tâche donnée\n",
    "# Utile pour pré-sélectionner un modèle avant de faire du fine-tuning sur \n",
    "# celui qui semble le plus performant pour la tâche envisagée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006cd8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\v\\anaconda3\\lib\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/e0/63/b428aaca15fcd98c39b07ca7149e24bc14205ad0f1c80ba2b01835aedde1/pip-23.3-py3-none-any.whl.metadata\n",
      "  Downloading pip-23.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\v\\anaconda3\\lib\\site-packages (68.2.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\v\\anaconda3\\lib\\site-packages (0.41.2)\n",
      "Downloading pip-23.3-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.1 MB 660.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.1 MB 1.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-23.3\n",
      "Requirement already satisfied: torch in c:\\users\\v\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchdata in c:\\users\\v\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\v\\anaconda3\\lib\\site-packages (from torchdata) (1.26.16)\n",
      "Requirement already satisfied: requests in c:\\users\\v\\anaconda3\\lib\\site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\v\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers==4.27.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: datasets==2.11.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.11.0) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\v\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\v\\anaconda3\\lib\\site-packages (from responses<0.19->datasets==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\v\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.27.2) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "#!{sys.executable} -m pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1\n",
    "!{sys.executable} -m pip install --disable-pip-version-check torch torchdata\n",
    "!{sys.executable} -m pip install transformers==4.27.2 \n",
    "\n",
    "# Transformers est un package créé par HuggingFace en accès libre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b2ea8-3080-4b37-8878-dd68fbeb609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U datasets==2.14.6\n",
    "!pip install fsspec==2023.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8566eac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Set up Kernel and required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3052365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c11cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('we love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a598f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Summarize dialogue without prompt engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053450cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107c26e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199e4bfdcce342e59792e3e30a170dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/knkarthick--dialogsum to C:/Users/V/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477bc22e35824094bd1faca3d230c62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c7eb04557c48a7a8c99f316b3cca18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d956558b78834add88221cb124249cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cef706d2c942a79d9cd97ccadb5f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b942e76b940e42d987268872194aba6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/V/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d32dd9fcee4726bf07edada71fe919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset=load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f060c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE: \n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print a couple of dialogues \n",
    "\n",
    "example_indices=[40,200]\n",
    "dash_line='-'.join('' for x in range(100))\n",
    "\n",
    "for i,index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Exemple ',i+1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6543b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eaf6c19c7d46058f7c1d2438eba58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\V\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\V\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b85048656014941961f15c9462dce11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6778a229daf48669dd1b862dff59e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# Flan T5 peut faire beaucoup de choses, notamment du résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7c7462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1ea1f624bb48c59424e81e6452e2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a84d0d398c94dbea1fdc93809264e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f0cc6b69de4212b8a68cb2239d2d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e0a0a77c7e439f97553a6cfbef61a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687fd6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE: \n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE: \n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded=tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "sentence_decoded=tokenizer.decode(sentence_encoded[\"input_ids\"][0],skip_special_tokens=True)\n",
    "\n",
    "print('ENCODED SENTENCE: ')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE: ')\n",
    "print(sentence_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eddd5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WHITHOUT PROMPT ENGINEERING:\n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - WHITHOUT PROMPT ENGINEERING:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue=dataset['test'][index]['dialogue']\n",
    "    summary=dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs=tokenizer(dialogue,return_tensors='pt')\n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Exemple ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WHITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfb03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Summarize dialogue with an instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Zero-shot inference with an instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0b465e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue=dataset['test'][index]['dialogue']\n",
    "    summary=dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt=f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input constructed prompt instead of the dialogue\n",
    "    inputs=tokenizer(prompt,return_tensors='pt')\n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Exemple ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb92c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Zero-shot inference with the prompt template from Flan-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89694f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut tester différentes manières de fournir l'instruction au modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fd7113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT avec FLan-T5 pre-built prompt:\n",
      "Tom is late for the train.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT avec FLan-T5 pre-built prompt:\n",
      "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware. #Person1#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,index in enumerate(example_indices):\n",
    "    dialogue=dataset['test'][index]['dialogue']\n",
    "    summary=dataset['test'][index]['summary']\n",
    "    \n",
    "    prompt=f\"\"\"\n",
    "Dialogue:\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input constructed prompt instead of the dialogue\n",
    "    inputs=tokenizer(prompt,return_tensors='pt')\n",
    "    output=tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Exemple ',i+1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT avec FLan-T5 pre-built prompt:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e56b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ça ne marche pas mieux..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d395f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Summarize dialogue with one-shot and few-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6c4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 One-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On construit une liste d'exemples à fournir au modèle en début de prompt, avant de donner la discussion à résumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d08140ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt=''\n",
    "    for index in example_indices_full:\n",
    "        dialogue=dataset['test'][index]['dialogue']\n",
    "        summary=dataset['test'][index]['summary']\n",
    "        prompt+=f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    # Ci-dessus, les trois sauts de lignes après {summary} sont importants pour aider Flan-T5 à comprendre. \n",
    "    # D'autres modèles peuvent avoir d'autres motifs préférentiels\n",
    "    \n",
    "    dialogue=dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt+=f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ea9944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full=[40]\n",
    "example_index_to_summarize=200\n",
    "\n",
    "one_shot_prompt=make_prompt(example_indices_full,example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ffe658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "summary=dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs=tokenizer(one_shot_prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47978db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C'est un peu mieux..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4933769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Few-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4754304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full=[40,80,120]\n",
    "example_index_to_summarize=200\n",
    "\n",
    "few_shot_prompt=make_prompt(example_indices_full,example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f56f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary=dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs=tokenizer(few_shot_prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db22f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Generative configuration parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6836502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "generation_config=GenerationConfig(max_new_tokens=50)\n",
    "#generation_config=GenerationConfig(max_new_tokens=50,do_sample=True,temperature=1.8)\n",
    "\n",
    "inputs=tokenizer(few_shot_prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'],generation_config)[0],skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c37d28ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "People are thinking up something to do on the computer in next few days. They are looking to make changes and improve their system systems and computers which are both already developed. Some people consider adding a painting program to the program. Another idea comes Regular\n"
     ]
    }
   ],
   "source": [
    "generation_config=GenerationConfig(max_new_tokens=50,do_sample=True,temperature=1.8)\n",
    "\n",
    "inputs=tokenizer(few_shot_prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'],generation_config)[0],skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29130199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bac à sable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a58f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Question:\n",
      "\"What is the average temperature in Paris in October, in °C ?\"\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Réponse:\n",
      "-45 ° celsius\n"
     ]
    }
   ],
   "source": [
    "generation_config=GenerationConfig(max_new_tokens=50,do_sample=True,temperature=0.7)\n",
    "\n",
    "inputs=tokenizer(\"What is the average temperature in Paris in October ?\",return_tensors='pt')\n",
    "output=tokenizer.decode(model.generate(inputs['input_ids'],generation_config)[0],skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print('Question:\\n\"What is the average temperature in Paris in October, in °C ?\"')\n",
    "print(dash_line)\n",
    "print(f'Réponse:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288a3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
