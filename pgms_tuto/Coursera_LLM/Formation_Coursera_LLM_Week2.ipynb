{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bae84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise la méthode de PEFT LoRA sur du résumé de dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73501e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Set up Kernel Load required dependencies, dataset and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Set up Kernel and required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69714e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\v\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\v\\anaconda3\\lib\\site-packages (68.2.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\v\\anaconda3\\lib\\site-packages (0.41.2)\n",
      "Requirement already satisfied: torch in c:\\users\\v\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchdata in c:\\users\\v\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\v\\anaconda3\\lib\\site-packages (from torchdata) (1.26.16)\n",
      "Requirement already satisfied: requests in c:\\users\\v\\anaconda3\\lib\\site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->torchdata) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\v\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers==4.27.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: datasets==2.11.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: evaluate==0.4.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: rouge_score==0.1.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: loralib==0.1.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: peft==0.3.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\v\\anaconda3\\lib\\site-packages (from transformers==4.27.2) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.11.0) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\v\\anaconda3\\lib\\site-packages (from datasets==2.11.0) (0.13.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\v\\anaconda3\\lib\\site-packages (from rouge_score==0.1.2) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\v\\anaconda3\\lib\\site-packages (from rouge_score==0.1.2) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from rouge_score==0.1.2) (1.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\v\\anaconda3\\lib\\site-packages (from peft==0.3.0) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from peft==0.3.0) (2.0.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\v\\anaconda3\\lib\\site-packages (from peft==0.3.0) (0.23.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.11.0) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\v\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\v\\anaconda3\\lib\\site-packages (from requests->transformers==4.27.2) (2023.7.22)\n",
      "Requirement already satisfied: sympy in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft==0.3.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft==0.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft==0.3.0) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\v\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.27.2) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\v\\anaconda3\\lib\\site-packages (from nltk->rouge_score==0.1.2) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\v\\anaconda3\\lib\\site-packages (from nltk->rouge_score==0.1.2) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\v\\anaconda3\\lib\\site-packages (from pandas->datasets==2.11.0) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\v\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.3.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\v\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.3.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "#!{sys.executable} -m pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1\n",
    "!{sys.executable} -m pip install --disable-pip-version-check torch torchdata\n",
    "!{sys.executable} -m pip install transformers==4.27.2 datasets==2.11.0 \\\n",
    "    evaluate==0.4.0 rouge_score==0.1.2 loralib==0.1.1 peft==0.3.0 \n",
    "# Ces 4 modules sont nouveaux par rapport à Week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bba8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des composants nécessaires\n",
    "from datasets import load_dataset\n",
    "# AutoModelForSeq2SeqLM pour accéder à FLAN-T5\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Load dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41d5f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/V/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c673e4d7030941c4a207370fb0b6b7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name=\"knkarthick/dialogsum\"\n",
    "dataset=load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d0b1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0ff22a3a3c42468413727305c83ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\V\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\V\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5a680dab894374b7c7eb7f2456fca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87971ee6dc904dad93e3d2080ee23f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On peut choisir la taille du modèle en fonction de la performance de l'environnement\n",
    "model_name='google/flan-t5-small'\n",
    "original_model=AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fc3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une fonction pour montrer les différents paramètres du modèles, en particulier les trainables\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params=0\n",
    "    all_model_params=0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_model_params+=param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params+=param.numel()\n",
    "    return f\"trainable model parameters : {trainable_model_params} \\nall model parameters : {all_model_params} \\npercentage of trainable model parameters : {trainable_model_params/all_model_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cfe16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters : 76961152 \n",
      "all model parameters : 76961152 \n",
      "percentage of trainable model parameters : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd06319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Test the model with zero-shot inferency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869504a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Exemple \n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "How would you like to upgrade your computer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index=200\n",
    "dash_line='-'.join('' for x in range(100))\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "summary=dataset['test'][index]['summary']\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "# Input constructed prompt instead of the dialogue\n",
    "inputs=tokenizer(prompt,return_tensors='pt')\n",
    "output=tokenizer.decode(original_model.generate(inputs['input_ids'],max_new_tokens=200)[0],skip_special_tokens=True)\n",
    "    \n",
    "print(dash_line)\n",
    "print('Exemple ')\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Perform full fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Pre-process the dialog-Summary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertit ici les paires prompt-réponses en input d'entraînement pour le modèle\n",
    "# On souhaite que le format du training prompt soit :\n",
    "# Summarize the following conversation.\n",
    "# {dialog}\n",
    "# Summary:\n",
    "# On définit ci-dessous une fonction qui construit et tokenize cette instruction-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf97825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt=\"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt=\"\\n\\nSummary: \"\n",
    "    prompt=[start_prompt+dialogue+end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids']=tokenizer(prompt,padding='max_length',truncation=True, return_tensors='pt').input_ids\n",
    "    example['labels']=tokenizer(example['summary'],padding='max_length',truncation=True, return_tensors='pt').input_ids\n",
    "    return example\n",
    "# Le dataset contient 3 splits : train, validation, test\n",
    "# La fonction ci-dessus s'applique à l'ensemble\n",
    "tokenized_datasets=dataset.map(tokenize_function,batched=True)\n",
    "tokenized_datasets=tokenized_datasets.remove_columns(['id','topic','dialogue','summary'])\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66aea9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 13\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# On n'utilise qu'une partie du dataset pour préserver les ressources\n",
    "tokenized_datasets=tokenized_datasets.filter(lambda example, index : index % 1000 ==0, with_indices=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730fba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training : (13, 2)\n",
      "Validation : (1, 2)\n",
      "Test : (2, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 13\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Format des datasets\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training : {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation : {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test : {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b629ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Fine-tune the model with the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c72e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise le module Training de HuggingFace\n",
    "# On fixe un certain nombre d'hyper-paramètres selon des valeurs conventionnelles\n",
    "\n",
    "output_dir=f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args=TrainingArguments(output_dir=output_dir,\n",
    "                                learning_rate=1e-5,\n",
    "                                num_train_epochs=1,\n",
    "                                weight_decay=0.01,\n",
    "                                logging_steps=1,\n",
    "                                max_steps=1)\n",
    "\n",
    "trainer=Trainer(model=original_model,\n",
    "               args=training_args,\n",
    "               train_dataset=tokenized_datasets['train'],\n",
    "               eval_dataset=tokenized_datasets['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f96560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\V\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=54.0, metrics={'train_runtime': 7038.5145, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.0, 'total_flos': 1487124037632.0, 'train_loss': 54.0, 'epoch': 0.5})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avec 1 epoch et 1 max_steps, le modèle ne sera que très peu amélioré, \n",
    "# mais ça ne prend que \"quelques minutes\" à faire tourner\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ef86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde le modèle qu'on vient d'entraîner\n",
    "trainer.save_model('./FlanT5-after-fine-tune-231029')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb9a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Evaluate the model qualitatively (human evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'évaluation ROUGE\n",
    "original_model_results=rouge.compute(predictions=original_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "instruct_model_results=rouge.compute(predictions=instruct_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "                                     \n",
    "print(f'ORIGINAL MODEL:\\n{original_model_results})\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd3c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend le modèle qu'on vient d'entraîner en full fine-tuning\n",
    "instruct_model=AutoModelForSeq2SeqLM.from_pretrained('./FlanT5-after-fine-tune-231029',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697ee5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "Get your computer backed up.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "How would you like to upgrade your computer?\n"
     ]
    }
   ],
   "source": [
    "# On compare sa performance avec le modèle original\n",
    "\n",
    "index=200\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "summary=dataset['test'][index]['summary']\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "    \n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Evaluate the Model quantitatively (ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a38ba04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11bdc82e95545b9a2995eff8a0bcaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge=evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d620978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Is it OK to send an intra-office memorandum to...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: Please, please send the memo to all...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>You're not going to be a desk sucks.</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>I'm a car driver.</td>\n",
       "      <td>The traffic jam was so bad that I couldn't get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam was a long time ago.</td>\n",
       "      <td>The traffic jam was so bad that I couldn't get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>Taking the subway would be a lot less stressful.</td>\n",
       "      <td>The traffic jam was so bad that I couldn't get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>People aren't able to understand what happened.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1#: Getting married.</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>Brian, thanks for the birthday party.</td>\n",
       "      <td>Brian, how are you?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  Is it OK to send an intra-office memorandum to...   \n",
       "1  #Person1#: Please, please send the memo to all...   \n",
       "2               You're not going to be a desk sucks.   \n",
       "3                                  I'm a car driver.   \n",
       "4               The traffic jam was a long time ago.   \n",
       "5   Taking the subway would be a lot less stressful.   \n",
       "6    People aren't able to understand what happened.   \n",
       "7               Masha and Hero are getting divorced.   \n",
       "8                        #Person1#: Getting married.   \n",
       "9              Brian, thanks for the birthday party.   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0                                                ...  \n",
       "1                                                ...  \n",
       "2                                                ...  \n",
       "3  The traffic jam was so bad that I couldn't get...  \n",
       "4  The traffic jam was so bad that I couldn't get...  \n",
       "5  The traffic jam was so bad that I couldn't get...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9                                Brian, how are you?  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On prépare l'évaluation sur 10 dialogues\n",
    "\n",
    "dialogues=dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries=dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries=[]\n",
    "instruct_model_summaries=[]\n",
    "\n",
    "for _,dialogue in enumerate(dialogues):\n",
    "    prompt=f\"\"\"\n",
    "Summarize the following dialogue.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries=list(zip(human_baseline_summaries,original_model_summaries,instruct_model_summaries)) \n",
    "df=pd.DataFrame(zipped_summaries,columns=['human_baseline_summaries','original_model_summaries','instruct_model_summaries'])\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59c816fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is it OK to send an intra-office memorandum to all employees?', '#Person1#: Please, please send the memo to all employees before 4 pm.', \"You're not going to be a desk sucks.\", \"I'm a car driver.\", 'The traffic jam was a long time ago.', 'Taking the subway would be a lot less stressful.', \"People aren't able to understand what happened.\", 'Masha and Hero are getting divorced.', '#Person1#: Getting married.', 'Brian, thanks for the birthday party.']\n",
      "['Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.', 'In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.', 'Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.', '#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.', \"#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.\", '#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.', '#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.', '#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.', '#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched', '#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.']\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(original_model_summaries)\n",
    "print(human_baseline_summaries[0:len(original_model_summaries)])\n",
    "print(len(original_model_summaries))\n",
    "print(len(human_baseline_summaries[0:len(original_model_summaries)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4262a77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIs it OK to send an intra-office memorandum to all employees?\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      2\u001b[0m                                     reference\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMs. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      3\u001b[0m                                     use_aggregator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m                                     use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "rouge.compute(predictions=['Is it OK to send an intra-office memorandum to all employees?'],\n",
    "                                    reference=['Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.'],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0ef4e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calcul de l'évaluation ROUGE\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m original_model_results\u001b[38;5;241m=\u001b[39mrouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39moriginal_model_summaries,\n\u001b[0;32m      3\u001b[0m                                     reference\u001b[38;5;241m=\u001b[39mhuman_baseline_summaries[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(original_model_summaries)],\n\u001b[0;32m      4\u001b[0m                                     use_aggregator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                                     use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m instruct_model_results\u001b[38;5;241m=\u001b[39mrouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39minstruct_model_summaries,\n\u001b[0;32m      7\u001b[0m                                     reference\u001b[38;5;241m=\u001b[39mhuman_baseline_summaries[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(instruct_model_summaries)],\n\u001b[0;32m      8\u001b[0m                                     use_aggregator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m                                     use_stemmer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\evaluate\\module.py:551\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Calcul de l'évaluation ROUGE\n",
    "original_model_results=rouge.compute(predictions=original_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "instruct_model_results=rouge.compute(predictions=instruct_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5824680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'instruct_model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Amélioration absolue en points de %\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAbsolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m improvement\u001b[38;5;241m=\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(instruct_model_results\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(original_model_results\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(instruct_model_results\u001b[38;5;241m.\u001b[39mkeys(),improvement):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'instruct_model_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Amélioration absolue en points de %\n",
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
    "improvement=(np.array(list(instruct_model_results.values()))-np.array(list(original_model_results.values())))\n",
    "for key,value in zip(instruct_model_results.keys(),improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71119ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Perform Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Set up the PEFT/LoRA model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# Rang r=32, relativement grand\n",
    "# Ci-dessous, TaskType.SEQ_2_SEQ_LM correspond à Flan-T5\n",
    "lora_config=LoraConfig(r=32,lora_alpha=32,\n",
    "                       target_modules=['q','v'],\n",
    "                       lora_dropout=0.05,\n",
    "                       bias=\"none\",\n",
    "                       task_type=TaskType.SEQ_2_SEQ_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model=get_peft_model(original_model,lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Train PEFT Adapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise à nouveau le module Training de HuggingFace\n",
    "# Le learning rate est supérieur à celui utilisé pour le full fine-tuning\n",
    "\n",
    "output_dir=f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args=TrainingArguments(output_dir=output_dir,\n",
    "                                auto_find_batch_size=True,\n",
    "                                learning_rate=1e-3,\n",
    "                                num_train_epochs=1,\n",
    "                                logging_steps=1,\n",
    "                                max_steps=1)\n",
    "\n",
    "peft_trainer=Trainer(model=peft_model,\n",
    "               args=peft_training_args,\n",
    "               train_dataset=tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraîne le nouveau modèle\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde le nouveau modèle\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ad123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quand on charge un Peft adapter, celui-ci a très peu de paramètres\n",
    "# On le \"fusionne\" avec un grand modèle pre-trained, ici Flan-T5\n",
    "# is_trainable=False pour indiquer qu'on veut utiliser le modèle PEFT adapté pour inférence uniquement, pas pour un\n",
    "# entraînement complémentaire. Ainsi, Pytorch ne charge pas les paramètres d'optimisation, ce qui libère de l'espace\n",
    "from peft import PeftModel, PeftConfig\n",
    "peft_model_base=AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base',torch_dtype=torch.bfloat16)\n",
    "tokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "peft_model=PeftModel.from_pretrained(peft_model_base,\n",
    "                                    peft_model_path,\n",
    "                                    torch_dtype=torch.bfloat16,\n",
    "                                    is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Evaluate the model qualitatively (human evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6475916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On compare sa performance avec les modèles précédents\n",
    "\n",
    "index=200\n",
    "dialogue=dataset['test'][index]['dialogue']\n",
    "summary=dataset['test'][index]['summary']\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    \n",
    "input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200,num_beams=1))\n",
    "peft_model_text_output=tokenizer.decode(peft_model_outputs[0],skip_special_tokens=True)\n",
    "    \n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec83457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Evaluate the Model quantitatively (ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4103e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prépare l'évaluation sur 10 dialogues\n",
    "\n",
    "dialogues=datasets['test'][0:10]['dialogue']\n",
    "human_baseline_summaries=datasets['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries=[]\n",
    "instruct_model_summaries=[]\n",
    "peft_model_summaries=[]\n",
    "\n",
    "for idx,dialogue in enumerate(dialogues):\n",
    "    prompt=f\"\"\"\n",
    "Summarize the following dialogue.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids=tokenizer(prompt,return_tensors='pt').input_ids\n",
    "    human_baseline_text_output=human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output=tokenizer.decode(original_model_outputs[0],skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    \n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output=tokenizer.decode(instruct_model_outputs[0],skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids,generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output=tokenizer.decode(peft_model_outputs[0],skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "    \n",
    "zipped_summaries=list(zip(human_baseline_summaries,original_model_summaries,instruct_model_summaries,peft_model_summaries)) \n",
    "df=pd.Dataframe(zipped_summaries,columns=['human_baseline_summaries','original_model_summaries','instruct_model_summaries','peft_model_summaries'])\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'évaluation ROUGE\n",
    "original_model_results=rouge.compute(predictions=original_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "instruct_model_results=rouge.compute(predictions=instruct_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "                                     \n",
    "peft_model_results=rouge.compute(predictions=peft_model_summaries,\n",
    "                                    reference=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_results})\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_results})\n",
    "print(f'PEFT MODEL:\\n{peft_model_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amélioration absolue en points de % par rapport à ORIGINAL MODEL\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "improvement=(np.array(list(peft_model_results.values()))-np.array(list(original_model_results.values())))\n",
    "for key,value in zip(peft_model_results.keys(),improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amélioration absolue en points de % par rapport à INSTRUCT MODEL\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
    "improvement=(np.array(list(peft_model_results.values()))-np.array(list(instruct_model_results.values())))\n",
    "for key,value in zip(peft_model_results.keys(),improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48854dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42671cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344c6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13eb92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06fe08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb88798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e6e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0bc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca191c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf745bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
