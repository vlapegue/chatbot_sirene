{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install --upgrade peft trl\n",
    "#!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets scipy protobuf #wandb\n",
    "!pip install -q ipywidgets==7.7.1\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Kernel after installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINE-TUNING DU MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe la liste de dictionnaires de conversations Sirene\n",
    "import os, sys, s3fs\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "fs.ls(\"vlapegue\")\n",
    "bucket = 'vlapegue/train_sirene'\n",
    "files = fs.ls(bucket)[-3:]\n",
    "fs.download(files[2],'train_sirene.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"train_sirene.csv\",split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On transforme la chaîne de caractères de messages en liste, (ce qui modifie l'ordre des entrées, a priori sans importance)\n",
    "import ast\n",
    "dataset = dataset.map(lambda x: {\"messages\": ast.literal_eval(x['messages'])} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=1024,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au cas où il faudrait appliquer un chat template à SFTTrainer (a priori pas nécessaire)\n",
    "# tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# print(tokenizer.decode(tokenized_chat[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Qui est l'actuel président de la République française ?\", return_tensors=\"pt\")\n",
    "output_generate=model.generate(**inputs,max_new_tokens=20, return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(inputs))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])\n",
    "print(generate_prompt(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=1234)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)  # test_size for testing\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "model_peft = accelerator.prepare_model(model_peft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable, total = model_peft.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "project = \"SFT-mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"messages\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        auto_find_batch_size=True,\n",
    "        evaluation_strategy=\"no\",\n",
    "        do_eval=False,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=8,\n",
    "        bf16=True,\n",
    "        warmup_steps=2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"outputs\",\n",
    "        #eval_steps=10,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=None,\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    "    ),\n",
    "    #data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SAUVEGARDE DU MODELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe le modèle Peft sauvegardé précédemment\n",
    "! mc cp s3/vlapegue/train_sirene/Mistral_Sirene_240718/Mistral_Sirene_240718 ./Mistral_Sirene_240718 --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    temperature=0,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    #torch_dtype=torch.float16,\n",
    "    #device_map={\"\": 0},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle de base\n",
    "messages = [{\"role\": \"user\", \"content\": \"Qui est l'actuel président de la République française ?\"}]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "output_generate=base_model.generate(model_inputs,max_new_tokens=80, temperature=0,return_dict_in_generate=True, output_scores=True,pad_token_id=tokenizer.eos_token_id)\n",
    "transition_scores = base_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length = 1 if base_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tokens':tokens,'probas': probas})\n",
    "df.to_csv(\"general_base_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle de base pour une question Sirene\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"J’aimerais obtenir mon avis de situation au répertoire pour avis et suis disponible si d’autres documents s’avéraient nécessaires.\"}]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "#inputs = tokenizer(\"J’aimerais obtenir mon avis de situation au répertoire pour avis et suis disponible si d’autres documents s’avéraient nécessaires.\", return_tensors=\"pt\")\n",
    "output_generate=base_model.generate(model_inputs,max_new_tokens=80, temperature=0,return_dict_in_generate=True, output_scores=True,pad_token_id=tokenizer.eos_token_id)\n",
    "transition_scores = base_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length = 1 if base_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tokens':tokens,'probas': probas})\n",
    "df.to_csv(\"sirene_base_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"./Mistral_Sirene_240718\"\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle fine-tuné\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Qui est l'actuel président de la République française ?\"}]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "output_generate=merged_model.generate(model_inputs,max_new_tokens=80, temperature=0,return_dict_in_generate=True, output_scores=True,pad_token_id=tokenizer.eos_token_id)\n",
    "transition_scores = merged_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#input_length = 1 if merged_model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "input_length = 1 if merged_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tokens':tokens,'probas': probas})\n",
    "df.to_csv(\"general_merged_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle fine-tuné pour une question Sirene\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"J’aimerais obtenir mon avis de situation au répertoire pour avis et suis disponible si d’autres documents s’avéraient nécessaires.\"}]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "output_generate=merged_model.generate(model_inputs,max_new_tokens=80, temperature=0,return_dict_in_generate=True, output_scores=True,pad_token_id=tokenizer.eos_token_id)\n",
    "transition_scores = merged_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#input_length = 1 if merged_model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "input_length = 1 if merged_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'tokens':tokens,'probas': probas})\n",
    "df.to_csv(\"sirene_merged_tokens.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle de base avec model_inputs\n",
    "messages = [{\"role\": \"user\", \"content\": \"Qui est l'actuel président de la République française ?\"}]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "output_generate=base_model.generate(model_inputs,max_new_tokens=80, temperature=0,return_dict_in_generate=True, output_scores=True,pad_token_id=tokenizer.eos_token_id)\n",
    "transition_scores = base_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#input_length = 1 if base_model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "input_length = 1 if base_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle de base avec inputs\n",
    "\n",
    "inputs = tokenizer(\"Qui est l'actuel président de la République française ?\", return_tensors=\"pt\")\n",
    "output_generate=base_model.generate(**inputs,max_new_tokens=80, temperature=0, return_dict_in_generate=True, output_scores=True)\n",
    "transition_scores = base_model.compute_transition_scores(output_generate.sequences, output_generate.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_length = 1 if base_model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "#input_length = 1 if base_model.config.is_encoder_decoder else model_inputs.size(dim=1)\n",
    "\n",
    "generated_tokens = output_generate.sequences[:, input_length:]\n",
    "\n",
    "tokens=[]\n",
    "probas=[]\n",
    "\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "\n",
    "# | token | token string | log probability | probability\n",
    "    tokens.append(tokenizer.decode(tok))\n",
    "    probas.append(np.exp(score.numpy()))\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
