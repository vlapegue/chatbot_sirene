{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL = nouvelle syntaxe pour rendre plus facile l'appel et la liaison de chaînes et d'agents\n",
    "\n",
    "# Méthodes classiques attachées aux chaînes : \n",
    "# invoke = à partir d'un input\n",
    "# stream = à partir d'un input, on renvoie une réponse\n",
    "# batch = à partir d'une liste d'inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1407bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade48311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1659adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# Le composant ci-dessous prend un message de chat et le transforme en string (simple!)\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8093f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a35dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un prompt template, on initialise le modèle et l'output parser\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée cette chaîne (syntaxe de type Linux)\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5843cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On appelle la chaîne, avec en input un dico contenant les éléments nécessaires (ici : \"topic\" dans le prompt template)\n",
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1225166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex chain : utiliser des docs annexes pour améliorer la génération de textes\n",
    "# Runnable Map pour envoyer les inputs de l'usager vers le prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a55f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf92cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise le retriever\n",
    "# Le vectorstore est composé de quelques phrases à peine\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec la fonction get_relevant_documents,\n",
    "# on obtient les documents les plus pertinents associés à une question, dans l'ordre de pertinence\n",
    "retriever.get_relevant_documents(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what do bears like to eat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans cette chaîne, les prompts suivront toujours le même template ci-dessous\n",
    "# Il y aura deux inputs pour le prompt : 'context' et 'question'\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63091388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le composant \"RunnableMap\" part d'un dictionnaire en input et retourne un autre dictionnaire, \n",
    "# au format souhaité par le composant suivant (prompt)\n",
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450e05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le contexte en input du prompt consiste en l'ensemble des documents pertinents de notre documentation,\n",
    "# repérés par le retriever\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfa0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour voir en détail ce que fait le composant \"RunnableMap\"\n",
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe777ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind and OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb613683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fonction d'Open AI\n",
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42828e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntaxe pour que le modèle de chat intègre la fonction ci-dessus\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4130dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51867312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La réponse est au format de sortie du modèle\n",
    "# On constate que 'content' est vide et \n",
    "# qu'il y a appel de la bonne fonction (dans les additional_kwargs, \n",
    "# avec récupération du bon code d'aéroport\n",
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ajoute d'autres fonctions\n",
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met à jour le modèle\n",
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a660a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met à jour la chaîne\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le modèle a bien reconnu la bonne fonction dans l'exemple ci-dessous\n",
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c982f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallbacks\n",
    "# C'est une sorte d'auto-réparation\n",
    "# Cas d'usage ci-dessous : demander à un modèle de sortir un JSON en output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les llms de cette library sont un peu moins bons que les ChatModels utilisés jusqu'ici dans ce notebook\n",
    "from langchain.llms import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c79258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-davinci est un vieux modèle d'OpenAI\n",
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1000, \n",
    "    model=\"text-davinci-001\"\n",
    ")\n",
    "# Chaine simple pour sortir un JSON\n",
    "simple_chain = simple_model | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'ouput n'a pas vraiment une structure de JSON\n",
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13695e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La preuve : json.loads ne le comprend pas\n",
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b454cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les nouveaux modèles d'OpenAI sont bons pour sortir des formats de JSON valides\n",
    "# Il faut juste ajouter un parser pour convertir l'output du ChatModel en string (le texte du message)\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07713d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c38101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback : pour switcher d'une première chaîne à une liste de chaînes de secours en cas d'erreur de la première\n",
    "final_chain = simple_chain.with_fallbacks([chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4279ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb23ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch : on met en input une liste d'inputs\n",
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream : on fait apparaître les outputs les uns après les autres\n",
    "# Utiles pour avoir les premiers éléments de réponse quand les suivants tardent à venir\n",
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b43e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthodes asynchronisées (d'où le await)\n",
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
