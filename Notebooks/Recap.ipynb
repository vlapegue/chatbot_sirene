{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd959a0d-589b-43dd-81ad-8ba870c44c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "import os\n",
    "import sys\n",
    "! pip install langchain \n",
    "! pip install pypdf\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install --disable-pip-version-check torch torchdata\n",
    "!pip install -U transformers\n",
    "!pip install -U datasets==2.14.6\n",
    "!pip install fsspec==2023.9.2\n",
    "!pip install lark\n",
    "!pip install gpt4all\n",
    "! pip install accelerate\n",
    "# au terminal : huggingface-cli login puis token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aae68a-5986-4497-8feb-30ef7247b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "from transformers import pipeline\n",
    "from transformers import pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "import os\n",
    "import s3fs\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8cb9f-9c45-4807-93d5-65d2f0cbcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01029666-51a3-420d-9cef-9d9de14e55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"bdc.txt\")\n",
    "pages_txt=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c65458-bdc5-413f-b7a0-fec66e0a0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"###\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"#\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e32b63-c455-4afd-bcf5-511114f6135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95f7d2-094e-47bc-b859-b8b6a8daf1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits = markdown_splitter.split_text(pages_txt[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4de22-00b2-4033-b329-f7f1c973b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(md_header_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdccdaf-6724-496e-9820-9ebe3b7329b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(md_header_splits)):\n",
    "    print(f'Contenu n° {i} : {len(md_header_splits[i].page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ede5aa-7c00-4b4a-9456-60fb4d7fcc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06cbfa-3c17-4ad3-8788-34389d019eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partie embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675b30d-c2f6-4450-9eab-dbc0036b5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_multilingual = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcfa264-69d2-419e-9e7e-f7b84a632d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0294e07-fa19-464e-ab58-ac0524be926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vider le dossier à la main ou utiliser cette ligne si besoin\n",
    "!rm -rf ./chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6399f1-13c0-4af0-b24c-91c5e8c844a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=md_header_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cadae-b941-44f8-9af1-b3a1a1623c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933d7b8-4611-4aec-a213-48aa7abdd061",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Je n'arrive pas à obtenir mon avis de situation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacfba0-8147-4dc3-9270-e656799f5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe9d70-881b-4660-b100-b82066a3ae48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943b482-bebb-4a56-81f4-3239832f3170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c0e77-38ee-4ac5-bb5b-45034bb4a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de MMR : équilibre entre pertinence et diversité des documents retrouvés\n",
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac193f7-9e8d-478c-b733-75a9bfc3f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser automatiquement les infos présentes dans les metadata : \n",
    "# we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "# 1. The `query` string to use for vector search\n",
    "# 2. A metadata filter to pass in as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d21b50-1150-43e3-8a1b-f28598568863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important de bien remplir \"description\" vu que ce sera lu par le LLM\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Header 1\",\n",
    "        description=\"Le thème général auquel la question se rattache\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Header 2\",\n",
    "        description=\"La catégorie au sein du thème général\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Header 3\",\n",
    "        description=\"La sous-catégorie à laquelle la question est rattachée\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b30800-5aef-4921-9fdb-9084de796531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt llama chat\n",
    "template_chat = \"\"\"<s>[INST] <<SYS>>\n",
    "\\n\n",
    "Vous êtes un assistant conversationnel cordial et honnête, qui répond, uniquement en langue française, aux questions ou aux problèmes posés par un usager. Si vous ne connaissez pas la réponse, répondez simplement que vous ne savez pas, n'essayez pas d'inventer la réponse. \n",
    "\\n<</SYS>>\n",
    "\\n\n",
    "À l'aide du contexte ci-dessous, répondez, uniquement en langue française, au problème suivant posé par un usager : {question}\n",
    "\\n\\n\n",
    "Contexte : \n",
    "\\n\n",
    "{context}\n",
    "[/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT_chat = PromptTemplate.from_template(template_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f9edf-f570-4c46-9aea-fa15a78d03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "template = \"\"\"Vous êtes un assistant conversationnel cordial et honnête, qui répond, uniquement en langue française, aux questions ou aux problèmes posés par un usager. Si vous ne connaissez pas la réponse, répondez simplement que vous ne savez pas, n'essayez pas d'inventer la réponse.\n",
    "\n",
    "À l'aide du contexte ci-dessous, répondez au problème suivant posé par un usager : {question}\n",
    "\n",
    "Contexte : \n",
    "\\n\n",
    "{context}\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8848a6a-f0ce-4fcf-9f64-9adbe6f41758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on utilise un modèle téléchargé, par exemple nous-hermes-llama2-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b2bcc-7979-4ffa-b0c7-a9b04730c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbaafb-6052-42ad-a575-20e4298a4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"vlapegue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d816a9-ebf1-4567-820e-a127479c134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'vlapegue/nous-hermes-llama2-13b'\n",
    "files = fs.ls(bucket)[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a839760-c443-43bc-a74c-a7b21ebdfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d96fbd-0939-43df-8f3a-966f8c9ead3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.download(files[1],'nous-hermes-llama2-13b.Q4_0.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40151b0-0a2d-441b-9795-a40ed9f53f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model='./nous-hermes-llama2-13b.Q4_0.gguf',temp=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a637402-564d-4c4b-85a0-b16f2c50a67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5968075-1b51-4917-bca0-66b2c99b0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True, verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f41f7e-7369-423a-997c-e1560e55517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c05b6-f963-4d9e-a9c8-bddc4a12476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e791f7-07fd-4dab-9171-086e3444642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grâce à la chaîne, on peut aussi voir quels documents ont été les plus pertinents pour répondre\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23355441-3c8a-4632-905e-e90eb7a2285e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfd15a-b140-4753-aca1-0250a69381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on utilise un modèle via le pipeline de HuggingFace, par exemple bigscience/bloomz-560m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29d8a2-b00f-4e10-8a53-6f1abe1ef77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_bloom = pipeline('text-generation', model = 'bigscience/bloomz-560m', min_new_tokens=60)\n",
    "llm = HuggingFacePipeline(pipeline=gen_bloom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b55ad-221b-439b-a571-13160c617786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True, verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5d7fb-70e6-47f1-b741-aec507187ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6e529-487c-4e34-ac46-ffe30b7f49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277cfae9-b53e-410a-81f8-39b28ee5acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grâce à la chaîne, on peut aussi voir quels documents ont été les plus pertinents pour répondre\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6e2bc-5536-4156-ac76-3132bd739ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6486e-37b7-4d08-a472-583e20267548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain avec un modèle orca mini\n",
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e587b47-1c3f-4d8e-8347-1a432bd1c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model='orca-mini-3b-gguf2-q4_0.gguf',temp=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b39d33-e70b-439d-97c0-162e445630aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True, verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e1a58-2fac-41cd-ad58-e7532ed71f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff04053-1df2-4c11-9d6a-933d5b546efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118bb64-8172-489c-8f85-cf026e751b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grâce à la chaîne, on peut aussi voir quels documents ont été les plus pertinents pour répondre\n",
    "for i in range(len(result[\"source_documents\"])):\n",
    "    print(result[\"source_documents\"][i],'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a77b2-eec5-411c-aeae-823c846aab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d28644-05cc-49b9-a2a5-91206bfcb6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f870a71-e247-4b60-9235-9798161d6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34fc8e3-eb3e-4e7c-8011-e82741730c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_llama = pipeline('text-generation', model = model, tokenizer=tokenizer,min_new_tokens=60)\n",
    "llm = HuggingFacePipeline(pipeline=gen_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22efc8-f2ad-41cb-8b91-a976d88d0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True, verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533b247-eb28-4aa3-99c9-0abb1721ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2a1e8-b215-46a5-ae75-983b7c45e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cbb52-d78b-4193-95a7-396a4a89911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3df60-4ffd-464a-907b-fc9e81d98824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec llama2 chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad446f34-55d5-46aa-a0b6-761d13e19f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",do_sample=True, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a626cc-1581-4164-89ce-5ef33f9ba5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_llama_chat = pipeline('text-generation', model = model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=gen_llama_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab58f8e-47e8-42ff-9ec2-15f3b5a7da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True, verbose=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_chat}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbd592-d3c9-457f-8988-cd348a30295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Où puis-je obtenir mon avis de situation ?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c087f-77cb-4a99-8967-d4956b3ac843",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994cfb1-88c1-4ea9-a9f4-f0c5471da8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c42b8-afc2-4653-b9fd-ccab0867ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion de la mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3a3e6-ffda-412c-812a-8c99b4783f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On garde en mémoire l'historique des messages\n",
    "# Return_messages = True signifie qu'on met les messages passés sous forme de liste, \n",
    "# et non de la forme d'un simple texte\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab951aff-dd62-41ee-857c-7948c7c7469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le module ConversationalRetrievalChain gère la mémoire\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c95877-7855-4769-b0eb-6074b3949a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Où puis-je obtenir mon avis de situation ?\"\n",
    "result = qa_memory({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c688580-64e6-4a8a-bc50-9129fe22565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7f3ee-239c-4631-a72f-498217decf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Je ne sais pas si mes données sont diffusées\"\n",
    "result = qa_memory({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df233a-70c1-4029-b7fe-e4cad903afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029246db-e29f-4c68-852a-f317ba793c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069076a-5df0-420a-b819-ddc2dc16eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot that works on your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e56176-a8fc-4a08-a01d-edc1901ee019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chatbot code has been updated a bit since filming. \n",
    "# The GUI appearance also varies depending on the platform it is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd6007-7303-4182-b218-708242c17b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit ici toute la chaîne qui part de la question, jusqu'à la réponse du chatbot\n",
    "# Pour des raisons d'ergonomie du chatbot, \n",
    "# on ne met pas la memory dans ce ConversationalRetrievalChain\n",
    "def load_db(chain_type, k):\n",
    "    # load documents\n",
    "    loader = TextLoader(\"bdc.txt\")\n",
    "    documents = loader.load()\n",
    "    # split documents - ajouter éventuellement dans les options separator=\"(?<=\\. )\"\n",
    "    headers_to_split_on = [\n",
    "    (\"###\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"#\", \"Header 3\")]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on)\n",
    "    docs = markdown_splitter.split_text(documents[0].page_content)\n",
    "    # define embedding\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    # create vector database from data\n",
    "    vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    # définir au préalable le modèle qu'on veut utiliser\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm, \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73aa5da-83f3-4e84-af9c-bf792b98186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.qa = load_db(\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File\")\n",
    "        else:\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File\")\n",
    "\n",
    "# C'est ci-dessous qu'on ajoute la memory\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344b0b6-f10e-4564-95c6-ebb6dd790695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b003ac7-595f-4bfd-969d-0d0afed6d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = cbfs()\n",
    "\n",
    "file_input = pn.widgets.FileInput(accept='.txt')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(cb.clr_history)\n",
    "inp = pn.widgets.TextInput( placeholder='Posez votre question ici…')\n",
    "\n",
    "bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "jpg_pane = pn.pane.Image( './img/LogoInsee.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column(\n",
    "    pn.panel(cb.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(cb.get_sources ),\n",
    ")\n",
    "tab3= pn.Column(\n",
    "    pn.panel(cb.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab4=pn.Column(\n",
    "    pn.Row( file_input, button_load, bound_button_load),\n",
    "    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n",
    "    pn.layout.Divider(),\n",
    "    pn.Row(jpg_pane.clone(width=400))\n",
    ")\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305916b6-0db1-4f9c-8302-1ae2f2d9e918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80657b4d-d681-4578-8dcd-2748163fd775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeef1a1-ddc5-43ed-bf27-3718d911f7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8d3a5-6418-4736-93a3-1e15a4e92519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12795cff-cb6d-4feb-929e-2436cb94811f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dde45-e91b-43d8-b400-d152f854776f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876efb8c-02f2-4838-a52e-82ac2a75656a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98bdf9-7f09-459f-9994-27e102cafda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdee844-b04a-4589-bdcf-e4d55da05bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2e1df-c179-4fac-961c-2002885d152b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0364961-16cd-4ade-9c04-5f8c3e6e6c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0427bd-0b56-444e-b894-9b5ef6ecdae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf37333-f426-4cbb-a2ec-ef1ba4c14a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0e2a5-ba97-4d56-b31a-ccd88e0cde84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6541f2a-9c37-41e4-b490-cb28ab2a1954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e3cc3-31ce-4dfe-964f-12a049c19cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fe58b-ffdc-4ee5-9bca-a95796856e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b99b6-a70b-4b8c-9ef3-eddcf2e99f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
