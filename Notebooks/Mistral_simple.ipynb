{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages\n",
    "import os\n",
    "import sys\n",
    "import s3fs\n",
    "from llama_cpp import Llama\n",
    "from langchain.callbacks.manager import CallbackManager \n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_verbose\n",
    "from langchain.llms import LlamaCpp\n",
    "set_verbose(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "class StreamDisplayHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container, initial_text=\"\", display_method='markdown'):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "        self.display_method = display_method\n",
    "        self.new_sentence = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.new_sentence += token\n",
    "\n",
    "        display_function = getattr(self.container, self.display_method, None)\n",
    "        if display_function is not None:\n",
    "            display_function(self.text)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid display_method: {self.display_method}\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs) -> None:\n",
    "        self.text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "bucket = 'vlapegue/mistral-7b-instruct-v0.2.Q4_K_M'\n",
    "files = fs.ls(bucket)[-3:]\n",
    "fs.download(files[1],'mistral-7b-instruct-v0.2.Q4_K_M.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    # Verbose is required to pass to the callback manager\n",
    "n_batch = 512\n",
    "llm = Llama(\n",
    "    model_path='./mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
    "    n_gpu_layers=0,\n",
    "    max_tokens = 8000,\n",
    "    temperature = 0.1,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,\n",
    "    use_mlock=True,\n",
    "    n_ctx=2048,\n",
    "    callback_manager=callback_manager,\n",
    "    n_threads=8,\n",
    "    verbose=True,\n",
    "    streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(query):\n",
    "    output=llm(query,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=True\n",
    ")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=run_mistral, \n",
    "    callback_user=\"Mistral\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mistral(\"Q: Who is the president of USA? A: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\n",
    "   \"Q: USA president in 1993 ? A:\",\n",
    "   max_tokens=32,\n",
    "   stop=[\"Q:\", \"\\n\"],\n",
    "   echo=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLama2\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "bucket = 'vlapegue/llama-2-7b-chat.Q5_K_M'\n",
    "files = fs.ls(bucket)[-3:]\n",
    "fs.download(files[1],'llama-2-7b-chat.Q5_K_M.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    # Verbose is required to pass to the callback manager\n",
    "n_batch = 512\n",
    "llm_llama2 = LlamaCpp(\n",
    "    model_path='./llama-2-7b-chat.Q5_K_M.gguf',\n",
    "    n_gpu_layers=0,\n",
    "    max_tokens = 8000,\n",
    "    temperature = 0.1,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,\n",
    "    use_mlock=True,\n",
    "    n_ctx=2048,\n",
    "    callback_manager=callback_manager,\n",
    "    n_threads=8,\n",
    "    verbose=True,\n",
    "    streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_llama2(\n",
    "   \"Q: USA president in 1993 ? A:\",\n",
    "   max_tokens=32,\n",
    "   stop=[\"Q:\", \"\\n\"],\n",
    "   echo=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
